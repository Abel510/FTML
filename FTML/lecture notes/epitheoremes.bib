@article{Defazio2014,
abstract = {In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
archivePrefix = {arXiv},
arxivId = {1407.0202},
author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
eprint = {1407.0202},
file = {:Users/nico/Desktop/enseignement/epita/doc/SAGA/Defazio_NIPS2014.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {1646--1654},
title = {{SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives}},
volume = {2},
year = {2014}
}
@article{Bach2021,
author = {Bach, Francis},
file = {:Users/nico/Desktop/enseignement/epita/doc/francis/ltfp_book.pdf:pdf},
journal = {Book Draft},
pages = {229},
title = {{Learning Theory from First Principles Draft}},
year = {2021}
}
@article{Allaire2012,
author = {Allaire, Gr{\'{e}}goire},
file = {:Users/nico/Desktop/enseignement/epita/doc/Allaire/EXT_1255_8.pdf:pdf},
journal = {{\'{E}}ditions de l'{\'{E}}cole Polytechnique},
number = {2},
pages = {480},
title = {{Analyse num{\'{e}}rique et optimisation Une introduction {\`{a}} la mod{\'{e}}lisation math{\'{e}}m{\'{e}}tique et {\`{a}} la simulation num{\'{e}}rique}},
url = {http://www.cmap.polytechnique.fr/$\sim$allaire/livre2.html},
year = {2012}
}
@article{Schmidt2013,
abstract = {We analyze the stochastic average gradient (SAG) method for optimizing the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method's iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate than black-box SG methods. The convergence rate is improved from O(1/k) to O(1 / k) in general, and when the sum is strongly-convex the convergence rate is improved from the sub-linear O(1 / k) to a linear convergence rate of the form O($\rho$k) for $\rho$< 1. Further, in many cases the convergence rate of the new method is also faster than black-box deterministic gradient methods, in terms of the number of gradient evaluations. This extends our earlier work Le Roux et al. (Adv Neural Inf Process Syst, 2012), which only lead to a faster rate for well-conditioned strongly-convex problems. Numerical experiments indicate that the new algorithm often dramatically outperforms existing SG and deterministic gradient methods, and that the performance may be further improved through the use of non-uniform sampling strategies.},
archivePrefix = {arXiv},
arxivId = {1309.2388},
author = {Schmidt, Mark and {Le Roux}, Nicolas and Bach, Francis},
doi = {10.1007/s10107-016-1030-6},
eprint = {1309.2388},
file = {:Users/nico/Desktop/enseignement/epita/doc/SAG/francis SAG.pdf:pdf},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Convergence Rates,Convex optimization,First-order methods,Stochastic gradient methods},
number = {1-2},
pages = {83--112},
title = {{Minimizing finite sums with the stochastic average gradient}},
volume = {162},
year = {2013}
}
